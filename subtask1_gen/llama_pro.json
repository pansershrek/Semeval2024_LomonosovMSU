{
    "MODEL_NAME": "TencentARC/LLaMA-Pro-8B",
    "MODEL_OUTPUT": "/data/semeval2024/result/llama_pro_finetune_1",
    "MERGED_MODEL_PATH": "/data/semeval2024/llama_pro_weights_merged",
    "TRAIN_DATASET": "/data/semeval2024/dataset/new_instruct_train_1.json",
    "TEST_DATASET": "/data/semeval2024/dataset/instruct_validation_1.json",
    "DATASET": "conll2003",
    "TRAIN_PARAMS": {
        "MICRO_BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,
        "EPOCHS": 2,
        "LEARNING_RATE": 1e-5,
        "MAX_LEN": 4096,
        "DEVICE_MAP": "auto",
        "LOAD_IN_8BIT": true,
        "USE_FLASH_ATTENTION_2": true,
        "WARMUP_STEPS": 10,
        "REPORT_TO": "wandb",
        "FP16": true
    },
    "LORA_PARAMS": {
        "LORA_R": 8,
        "LORA_ALPHA": 16,
        "LORA_DROPOUT": 0.05,
        "TARGET_MODULES": [
            "q_proj", "k_proj", "v_proj", "o_proj"
        ]
    },
    "SAMPLING_PARAMS": {
        "TEMPERATURE": 1,
        "TOP_K": 50,
        "TOP_P": 1.0,
        "MAX_TOKENS": 4096,
        "BATCH_SIZE": 512
    },
    "SEED": 1979
}